import streamlit as st
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import CharacterTextSplitter
import openai
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from datetime import datetime
import sqlite3
import os
import tempfile

# ì»¤ìŠ¤í…€ HyperCLOVA X ì„ë² ë”© ë˜í¼
def HyperclovaEmbeddings(api_key=None, model="hyperclova-x-embedding", base_url=None):
    class _Wrapper:
        def __init__(self, api_key, model, base_url):
            self.model = model
            openai.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if base_url:
                openai.api_base = base_url.rstrip("/")
        def embed_documents(self, texts):
            resp = openai.Embedding.create(model=self.model, input=texts)
            return [d.embedding for d in resp.data]
    return _Wrapper(api_key, model, base_url)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë¦¬í¬íŠ¸ ê¸°ë°˜ ì¢…ëª© ì¶”ì²œ ì±—ë´‡", layout="centered")
st.title("ğŸ“Š ë¯¸ë˜ì—ì…‹ ë¦¬í¬íŠ¸ ê¸°ë°˜ ì¢…ëª© ì¶”ì²œ ì±—ë´‡")

# ì‚¬ì´ë“œë°”ì—ì„œ API ì •ë³´ ì…ë ¥
api_key = st.sidebar.text_input("ğŸ” API í‚¤ ì…ë ¥", type="password")
base_url = st.sidebar.text_input("ğŸŒ API Base URL", value="https://api.hyperclova.naver.com/v1")
if not api_key:
    st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = api_key
os.environ["OPENAI_API_BASE"] = base_url

# LLM ë° ì„ë² ë”© ì´ˆê¸°í™”
llm = ChatOpenAI(model_name="hyperclova-x-large", temperature=0.7,
                 openai_api_key=api_key, openai_api_base=base_url)
embeddings = HyperclovaEmbeddings(api_key=api_key, base_url=base_url)

# DB ì´ˆê¸°í™” ë° í”¼ë“œë°± ì €ì¥ í•¨ìˆ˜
def init_db():
    conn = sqlite3.connect("feedback.db")
    cur = conn.cursor()
    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS feedback (
                      id INTEGER PRIMARY KEY AUTOINCREMENT,
                      timestamp TEXT,
                      investor_type TEXT,
                      question TEXT,
                      answer TEXT
                   )\"\"\")
    conn.commit(); conn.close()

def save_feedback(investor_type, question, answer):
    conn = sqlite3.connect("feedback.db")
    cur = conn.cursor()
    cur.execute(\"\"\"INSERT INTO feedback (timestamp, investor_type, question, answer)
                   VALUES (?, ?, ?, ?)\"\"\", 
                (datetime.now().strftime("%Y-%m-%d %H:%M"), investor_type, question, answer))
    conn.commit(); conn.close()

# ë³´ê³ ì„œ ë§í¬ ë° PDF ì¶”ì¶œ
def fetch_report_links(limit=5):
    base = "https://securities.miraeasset.com"
    list_url = f"{base}/bbs/board/message/list.do?categoryId=1521"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(list_url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")
    links = []
    for a in soup.select("div.board_list a"):
        title, href = a.text.strip(), a.get("href")
        if href and "view.do" in href:
            links.append((title, base + href))
        if len(links) >= limit: break
    return links

def fetch_pdf_urls(report_links):
    base = "https://securities.miraeasset.com"
    headers = {"User-Agent": "Mozilla/5.0"}
    pdfs = []
    for title, detail_url in report_links:
        resp = requests.get(detail_url, headers=headers)
        soup = BeautifulSoup(resp.text, "html.parser")
        a = soup.find("a", href=lambda x: x and x.endswith(".pdf"))
        if a:
            href = a["href"]
            pdfs.append((title, href if href.startswith("http") else base + href))
    return pdfs

# QA ì²´ì¸ êµ¬ì„±
def build_qa_chain():
    docs = []
    for title, url in fetch_pdf_urls(fetch_report_links()):
        try:
            data = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}).content
            tmp = os.path.join(tempfile.gettempdir(), os.path.basename(url))
            open(tmp, "wb").write(data)
            docs.extend(PyPDFLoader(tmp).load_and_split())
        except Exception as e:
            st.error(f"ë³´ê³ ì„œ '{title}' ë¡œë”© ì‹¤íŒ¨: {e}")
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    vectordb = FAISS.from_documents(splitter.split_documents(docs), embeddings)
    return RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever())

# ë©”ì¸ ì‹¤í–‰
init_db()
user_type = st.selectbox("íˆ¬ì ì„±í–¥ì„ ì„ íƒí•˜ì„¸ìš”", ["ì„±ì¥","ë°°ë‹¹","ê°€ì¹˜","ë‹¨íƒ€"])
question = st.text_input("ê´€ì‹¬ ìˆëŠ” ì§ˆë¬¸ì´ë‚˜ ì¡°ê±´ì„ ì…ë ¥í•˜ì„¸ìš”")
if question:
    with st.spinner("ë¶„ì„ ì¤‘..."):
        ans = build_qa_chain().run(question)
    st.success("ğŸ“Œ ì¶”ì²œ ê²°ê³¼")
    st.write(ans)
    save_feedback(user_type, question, ans)
